---
title: "Analysis"
author: "Number Guessing Team"
date: "07/12/2020"
output:
  html_document:
    highlight: kate
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro

Some text here about why/what we're doing. Dataset

```{r}
# --- Setup --- #

# Load libraries
suppressMessages(library(tidyverse))
library(RColorBrewer)
library(knitr)
library(lme4)

# Load data from data processing 
guesses <- read.csv('../data/guesses_processed.csv')
games <- read.csv('../data/games_processed.csv')
users <- read.csv('../data/users_processed.csv')

# Create color palette
paired <- brewer.pal(6, "Paired")
color1 <- paired[1]
color2 <- paired[2]
color3 <- paired[3]

```


# 1. How optimal are player strategies?

How well do players approximate the optimal strategy (binary search)?

## 1.1. What proportion of users and guesses are optimal?

Roughly 1/3 of guesses were optimal.

```{r}

guesses %>%
  mutate(isOptimal = factor(ifelse(isOptimal == 1, "Optimal", "Sub-optimal"))) %>%
  group_by(isOptimal) %>%
  summarise(prop = n() / nrow(guesses),
            .groups = "drop") %>%
  ggplot(aes(x = isOptimal, y = prop, fill=isOptimal)) + 
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(
    title="What proportion of guesses were optimal?",
    x = "Optimality",
    y = "No. guesses",
    fill = "Optimality"
  )

```

A unimodal distribution with a mean at around 0.25, shows most users produce an optimal guess around a quarter of the time. However, a small number of users are consistently producing optimal guesses more than 80% of the time.

```{r}

users %>%
  ggplot(aes(x=propOptimal)) +
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Proportion of guesses which were optimal by user",
    x = "Proportion of guesses that were optimal",
    y = "Density"
  )

```

# 1.2. How optimal are non-optimal guesses?

We can estimate the optimality of non-optimal guesses by finding how much information is gained by the guess relative to how much information would have been gained by an optimal guess.

We can see that many guesses have an optimality of close to 1 (representing optimal guesses). The majority of guesses have an optimality between 0 and 1. The remaining guesses, with optimality of > 1, are "lucky guesses" (e.g. guessing "90" on attempt #1, when the target is 95, even though this would not be a good strategy in the long run).

```{r}

guesses %>%
  ggplot(aes(x = infoGain.norm)) + 
  geom_density() +
  theme_minimal() +
  labs(
    title="How much information did guesses gain compared to binary search?",
    x = "Information gain / IG of binary search",
    y = "Density"
  )

```

Here they are binned.

I think the NAs are interesting/worth exploring more. They seem to be caused by cases where there is only one number available to guess. The way I've calculated infoGain, there is really no "information" gained in this situation. If only one number is left, you know that's it. Maybe we can redefine things if we want to include these cases.

```{r}


guesses %>%
  mutate(ig.norm.bin = cut(infoGain.norm, breaks = c(-0.01, 0.5, 0.95, 1.05, 10),
         labels = c("Poor (<0.5)", "Fair (0.5-0.95)", "Optimal (0.95-1.05)", "Lucky (1.05+)"))
         ) %>%
  group_by(ig.norm.bin) %>%
  summarise(n=n()) %>%
  ggplot(aes(x = ig.norm.bin, y = n, fill = ig.norm.bin)) + 
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(
    title="Guesses by optimality, binned.",
    x = "Optimality",
    y = "No. guesses",
    fill = "Optimality"
  )

```



## How optimal are guesses over the course of a game?

```{r}
guesses %>%
  ggplot(aes(attemptNum, isOptimal-1/(upperBound-lowerBound))) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 1,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  theme_minimal() +
  labs(
    title="Proportion of optimal guesses for the n-th guess beyond chance",
    x = "N-th guess",
    y="Proportion of optimal guesses - chance of optimal guess"
  )
```
This shows how often the optimal guess was made on the n-th guess for a particular game beyond what might be expected by chance. 0 corresponds to performing at chance, negative number indicate performing below chance. This shows that, on average, the 9th guess and later are less often optimal than what might be expected by chance. This is because most games end before the 9th guess is madeâ€”therefore, players with many guesses tend to be poor players.
Of note is that the first guess tends to be optimal more often than others. The optimal first guess is always 50, a salient number.


# How do player strategies change over time?

## Does player performance improve?

```{r}

games %>%
  ggplot(aes(game_index, numGuesses)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  theme_minimal() + 
  labs(
    title="Number of guesses for n-th game of user",
    x = "No. previous games played by user",
    y = "Number of guesses"
  )
  

```

Shows the mean number of guesses needed to complete each game against the "game index", or the number of games the user had played up until this point.

There does seem to be a downward trend but it's not as dramatic as I was expecting. Maybe this is because people who play for a long period of time are not very good (e.g. children, definition of madness etc), or because no_guesses is too weak of a measure (it can still be tough to get the number quickly even if you are using a good strategy).

```{r}

games %>%
  ggplot(aes(game_index, propOptimal)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  theme_minimal() + 
  ylim(0,1) +
  labs(
    title="Proportion of optimal guesses for n-th game of user",
    x = "No. previous games played by user",
    y = "Proportion of optimal guesses"
  )
  

```
Likewise, the proportion of optimal guesses markedly increases with the total number of games.

```{r}

games %>%
  ggplot(aes(game_index, infoGain.mean)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  theme_minimal() + 
  labs(
    title="Mean information gain per guess for n-th game of user",
    x = "No. previous games played by user",
    y = "Mean information gain"
  )
  

```
The same cannot be said for the mean information gain, however. How come?

```{r}

m1 <- lm(numGuesses ~ game_index, data=games)
summary(m1)

```

A linear model shows a strong relationship between numGuesses and game_index (you expect to make one fewer guess about every 300 games).

```{r}

model.game_index.guesses.null <- lmer(numGuesses ~ (1 + game_index | user), data=games)

summary(model.game_index.guesses.null)

model.game_index.guesses <- lmer(numGuesses ~ game_index + (1 + game_index | user), data=games)

summary(model.game_index.guesses)

anova(model.game_index.guesses, model.game_index.guesses.null)

```

A linear mixed effects model (which takes into account the variance between participants), shows a stronger relationship (it predicts you will make one fewer guess every 150 games). A likelihood ratio test against a null model shows the effect to be significant.






