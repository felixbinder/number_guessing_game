---
title: "Analysis"
author: "Number Guessing Team"
date: "07/12/2020"
output:
  html_document:
    highlight: kate
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro


## The dataset

The dataset comes from a number guessing game running on Amazon Alexa. Users play a game with the voice assistant where the game randomly chooses a number between 1 to 100 and the user tries to guess the number. After a guess, the user is told whether their guess was too low or too high. The dataset includes 380,000 guesses across 50,000 games and 14,000 players. The dataset was sourced from [kaggle](https://www.kaggle.com/sdobson46/higher-or-lower-game). The dataset includes unique player IDs for all games as well as the date of the game and the outcome of the game. The dataset also allows for analysis of a player’s first game and information about whether the player returned to play more games. This dataset only includes completed games with no more than 15 guesses. The dataset excludes all guesses that Alexa did not understand or were invalid because they were not in the 1 to 100 number range. The dataset includes repeated guesses and guesses that conflict with previous hints provided by Alexa.

### Codebook

Dataframe `games` from `games.csv` file on kaggle.

* 1 row per game played
* 50881 rows total
* 22 columns:
  * `gameId` (factor w/ 50881 levels): Unique identifier for each game
  * `user` (int): Unique identifier for each player
  * `startTime` and `finishTime` (POSIX datetime): start and finish times of games.
  * `duration` (int): Duration of game in seconds.
  * `targetNum` (int): The randomly selected number between 1-100 that the player needs to guess
  * `numGuesses` (int): Number of guesses the player needed to guess the target number
  * `guess1` - `guess15` (int): The numbers which the user guessed.
    

## Research questions

Our primary research questions are:

1. How well do players approximate the optimal strategy?
  1.1. What proportion of users adopt binary search (the optimal strategy)?
  1.2. How does participant performance relate to this optimal benchmark?
  
2. How do players’ strategies change and/or improve over time?
  2.1 Do players' responses improve over time (as measured by the information gain of each guess)?
  2.2 Do players adopt or better approximate binary search over time or do they improve by adopting other strategies?

3. What other strategies do players use and how can we characterize them?
  * Are there other patterns in players' guesses that imply other strategies?
  * How can we identify and categorize these strategies?
  * Are strategies reliable and rational? Or do they expose irrational biases in players' reasoning?
  
  
**TODO*: Ed mentioned he wanted us to look at the distribution of career lengths etc. We have these plots in the data_summary, but do you think we want to include any of that here/ in the writeup?


```{r}
# --- Setup --- #

# Load libraries
suppressMessages(library(tidyverse))
library(RColorBrewer)
library(knitr)
library(lme4)

# Load data from data processing 
guesses <- read.csv('../data/guesses_processed.csv')
games <- read.csv('../data/games_processed.csv')
users <- read.csv('../data/users_processed.csv')

# Load simulation data
guesses.binary = read.csv('../data/guesses_binary_search.csv')
guesses.random = read.csv('../data/guesses_random_search.csv')

# Create color palette
paired <- brewer.pal(6, "Paired")
color1 <- paired[1]
color2 <- paired[2]
color3 <- paired[3]

color.binary = "green"
color.random = "red"
```


# 1. How optimal are player strategies?

The guessing game affords an optimal strategy: binary search. At each guess, the player ought to select a guess which will eliminate the largest proportion of numbers from the remaining candidate numbers. Because the player has no information about where in the range of candidates the target number will fall, the best strategy is to pick the number at the centre of the range. In this way, roughly half of the candidates will be removed at each guess.

We define optimal search as `floor((upper+lower)/2)` where `upper` is the upper bound of the candidate range, and lower is the lower bound. At the start of the game, players have 100 candidates to choose among, so the optimal guess is 51 (or 50, which eliminates the same number of candidates on average: 49.5). The next guess will be 75 or 25 depending on whether the target number is above or below 51. The process continues, roughly halving the number of candidates at each guess until the target number is guessed (either because it happened to be in the centre of the range, or because only one number is left).

Using binary search, some numbers are easy to guess (50 requires only one guess), while others are hard to guess. However, no number needs more than 7 guesses to get correctly under binary search.

```{r}
guesses.binary %>% group_by(targetNum) %>% 
  summarise(numGuesses = n()/length(unique(gameId))) %>% 
  ggplot(aes(x = factor(targetNum), y = numGuesses)) + 
  geom_bar(fill=color.binary, stat = "identity") +
  theme_minimal() +
  labs(
    title="Number of guesses needed for target number under binary search",
    x = "Target number",
    y = "Number of guesses required"
  )
```

```{r}
guesses.random %>% group_by(targetNum) %>% 
  summarise(numGuesses = n()/length(unique(gameId))) %>% 
  ggplot(aes(x = factor(targetNum), y = numGuesses)) + 
  geom_bar(fill=color.random, stat = "identity") +
  theme_minimal() +
  labs(
    title="Number of guesses needed for target number under random search",
    x = "Target number",
    y = "Number of guesses required"
  )
```
```{r}
guesses %>% group_by(targetNum) %>% 
  summarise(numGuesses = n()/length(unique(gameId))) %>% 
  ggplot(aes(x = factor(targetNum), y = numGuesses)) + 
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    title="Number of guesses for target number",
    x = "Target number",
    y = "Number of guesses required"
  )
```

```{r}
guesses %>% group_by(targetNum) %>% 
  summarise(numGuesses = n()/length(unique(gameId))) %>% 
  ggplot(aes(x = factor(targetNum), y = numGuesses)) + 
  geom_bar(stat = "identity") +
  geom_point(data = guesses.binary %>% 
             group_by(targetNum) %>% 
             summarise(numGuesses = n()/length(unique(gameId))),
    stat = "identity",
    color = color.binary) +
  geom_point(data = guesses.random %>% 
             group_by(targetNum) %>% 
             summarise(numGuesses = n()/length(unique(gameId))),
    stat = "identity",
    color = color.random) +
  theme_minimal() +
  labs(
    title="Number of guesses for target number",
    x = "Target number",
    y = "Number of guesses required"
  )
```

## 1.1 Number of guesses per game

Following binary search, then, at attempt `n`, we expect the number of candidates to be `100 / 2 ^ n`. Because `2 ^ 7 > 100`, we can deduce that no game would take longer than 7 guesses.

**Todo? Confirm with simulation. They do this on the [kaggle notebook](https://www.kaggle.com/sdobson46/interesting-observations) in Python which we could adapt. Could do a nice plot of the distribution of no. of guesses it takes for each number with optimal search. Might also be nice to have an implementation written here.**


By plotting the distribution of guesses per game, we can see that many games are completed suboptimally.

```{r}


games %>%
  ggplot(aes(x = factor(numGuesses))) + 
  geom_bar(fill=color1, stat = "count") +
  geom_point(data = guesses.binary %>%
             group_by(gameId) %>%
             summarise(numGuesses = n()/length(unique(gameId))) %>%
             mutate(numGuesses = factor(as.numeric(numGuesses))),
    stat = "count",
    color = color.binary) +
  geom_point(data = guesses.random %>%
             group_by(gameId) %>%
             summarise(numGuesses = n()/length(unique(gameId))) %>%
             filter(numGuesses <= 15) %>% 
             mutate(numGuesses = factor(as.numeric(numGuesses))),
  stat = "count",
  color = color.random) +
  theme_minimal() +
  labs(
    title="Distribution of number of guesses per game",
    x = "Number of guesses",
    y = "Number of games"
  )

```


Overall, around `r round(games %>%filter(numGuesses > 7) %>%nrow() / nrow(games)*100,2)`% of games required more than 7 guesses, and this is a conservative estimate for games completed sub-optimally, as most games would require fewer than 7 guesses using binary search.

## Mean no. guesses by user

Do all users tend to take suboptimally long to complete games? Or do some players consistently complete games in < 7 guesses?

**NOTE: Here is where it might be nice to refer back to the fact that some players have completed many games and some very few**

Plotting the mean number of guesses/game for each user, grouping by the number of games each user played, we can see that there is a high spread across users. Some users who played 2-4 games consistently take more than 7 guesses to complete games.

**NOTE: more/better analysis needed here, although tbh we might not want to include this whole section as it is mostly setup/contextualisation to the optimality measures** We could use this section to show groupings of 5's and 10s for potential strategies (if we want to move it to question 3?)

```{r}

users %>%
  mutate(no_games_bin = cut(
        no_games,
        breaks = c(0, 1, 5, 200),
        labels = c("1", "2-4", "5+"),
        )
  ) %>%
  ggplot(aes(x=round(guesses.mean), fill=no_games_bin)) +
  geom_bar() +
  theme_minimal() +
  scale_fill_brewer(palette="Paired") +
  labs(
    title="Distribution of mean guesses per game across users",
    x = "Mean guesses per game",
    y = "Number of users",
    fill = "Games played"
  )

```


## 1.2. What proportion of users and guesses are optimal?

The number of guesses metric, however, is not very discriminative. Some games will take longer than other even using binary search. We can directly check if each guess is optimal by calculating the optimal guess at each decision point and checking whether the user's guess is equal to the optimal guess. Importantly, as discussed above wrt. 50/51, there are many situations in which two guesses will eliminate equal numbers of candidates on average, and so we treat both values (50 or 51) as optimal.

Firstly, we can plot the number of guesses that were optimal overall.

```{r}

guesses %>%
  mutate(isOptimal = factor(ifelse(isOptimal == 1, "Optimal", "Sub-optimal"))) %>%
  group_by(isOptimal) %>%
  summarise(prop = n() / nrow(guesses),
            .groups = "drop") %>%
  ggplot(aes(x = isOptimal, y = prop, fill=isOptimal)) + 
  geom_bar(stat = "identity") +
  geom_point(data = guesses.random %>%
              mutate(isOptimal = factor(ifelse(isOptimal == 1, "Optimal", "Sub-optimal"))) %>%
              group_by(isOptimal) %>%
              summarise(prop = n() / nrow(guesses),
                        .groups = "drop"),
             aes(x = isOptimal, y = prop),
             color= color.random)+
  geom_point(data = guesses.binary %>%
              mutate(isOptimal = factor(ifelse(isOptimal == 1, "Optimal", "Sub-optimal"))) %>%
              group_by(isOptimal) %>%
              summarise(prop = n() / nrow(guesses),
                        .groups = "drop"),
             aes(x = isOptimal, y = prop),
             color= color.binary)+
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(
    title="What proportion of guesses were optimal?",
    x = "Optimality",
    y = "Proportion of guesses",
    fill = "Optimality"
  )

```


```{r}
guesses %>%
  filter(isOptimal == 1) %>%
  nrow() / nrow(guesses)
```

33.3% of guesses were optimal overall, which indicates that users were not using binary search two thirds of the time.

Again we can ask how the proportion of optimal guesses distributes across users (are most users making optimal guesses 1/3 of the time, or are 1/3 of the users making consistently optimal guesses).

A unimodal distribution with a mean at around 0.25, shows most users produce an optimal guess around a quarter of the time. However, a small number of users are consistently producing optimal guesses more than 80% of the time.

**TODO: Do more users guess optimally than would be expected by chance for random guessing? I feel like we don't have many stats tests in here yet!**

```{r}

users %>%
  ggplot(aes(x=propOptimal)) +
  geom_density(color=color1, size=1) +
  geom_vline(xintercept = guesses.binary %>% pull(isOptimal) %>% mean(), color=color.binary)+
  geom_vline(xintercept = guesses.random %>% pull(isOptimal) %>% mean(), color=color.random)+
  theme_minimal() +
  labs(
    title="Proportion of guesses which were optimal by user",
    x = "Proportion of guesses that were optimal",
    y = "Density"
  )

```

Another interesting question is whether many of these optimal guesses are produced early on (when people know by rote to guess 50 > 25/75 etc), or whether they are produced later on, when fewer available options force a higher rate of optimal guesses by chance (as `n` decreases, the probability of choosing optimally ~`1/n` increases).

```{r}

guesses %>%
  mutate(isOptimal = factor(ifelse(isOptimal == 1, "Optimal", "Sub-optimal"))) %>%
  ggplot(aes(x = attemptNum)) + 
  geom_bar(aes(fill=isOptimal),position = "fill") +
  geom_point(data = guesses.binary %>%
                    group_by(attemptNum) %>% 
                    summarise(mean = mean(isOptimal)),
             aes(x=attemptNum,y=mean),
             color = color.binary)+
  geom_point(data = guesses.random %>%
                    group_by(attemptNum) %>% 
                    summarise(mean = mean(isOptimal)) %>% 
                    filter(attemptNum <= 15),
             aes(x=attemptNum,y=mean),
             color = color.random)+
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(
    title="What proportion of guesses were optimal?",
    x = "N-th guess",
    y = "Proportion of guesses",
    fill = "Optimality"
  )

```

Primarily, the latter effect seems to drive optimality, however, there is a higher number of optimal guesses on the first guess than the second and third, which might be due to participants learning (or intuiting) that 50 is a good first guess.

## How optimal are guesses over the course of a game?

```{r}
guesses %>%
  ggplot(aes(attemptNum, isOptimal-(1/(upperBound-lowerBound)))) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 1,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  stat_summary_bin(data = guesses.binary,
                   fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 1,
                   color=color.binary) +
  stat_summary_bin(data = guesses.random %>% filter(attemptNum < 15),
                   fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 1,
                   color=color.random) +
  theme_minimal() +
  labs(
    title="Proportion of optimal guesses for the n-th guess beyond chance",
    x = "N-th guess",
    y="Proportion of optimal guesses - chance of optimal guess")+
  scale_x_continuous(breaks = 1:15) 
```

This shows how often the optimal guess was made on the n-th guess for a particular game beyond what might be expected by chance. 0 corresponds to performing at chance, negative number indicate performing below chance. This shows that, on average, the 9th guess and later are less often optimal than what might be expected by chance. This is because most games end before the 9th guess is made—therefore, players with many guesses tend to be poor players.
Of note is that the first guess tends to be optimal more often than others. The optimal first guess is always 50, a salient number.

## 1.3. How much information is gained by guesses compared to optimal guesses?

The optimality of each guess is still a very crude measure. Some "non-optimal" guesses may be very close to the optimal guess, and eliminate a simular number of candidates. We want to find a metric that is sensitive to the number of candidates which were eliminated by each guess, relative to how many would have been eliminated by an optimal guess.

We could use this measure directly: proportion of candidates eliminated divided by the proportion that would have been eliminated by the optimal guess:

```{r}

guesses %>%
  ggplot(aes(x = propExcluded.norm)) + 
  geom_density(color=color1, size=1) +
  geom_density(data = guesses.binary,
    color=color.binary, size=1) +
  geom_density(data = guesses.random,
    color=color.random, size=1) +
  theme_minimal() +
  ylim(0,4)+
  labs(
    title="Proportion of candidates eliminated by relative to optimal guess",
    x = "Relative proportion eliminated",
    y = "Density"
  )

```

(N.B. I'm not entirely sure what the downside of doing this would be, but Ed suggested using info gain / optimal info gain so I think we should do that).

We can also define the amount of **information** gained by each guess. In Information Theory, "the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) of a random variable is the average level of "information", "surprise", or "uncertainty" inherent in the variable's possible outcomes." In our case, because each candidate is as likely as the next, entropy is directly proportional to the number of candidates (the more candidates remaining, the more uncertain you are about which is correct).

We can calculate the entropy of our available candidates by summing `-P(x) * log2(P(x))` for each `x` where `P(x)` is the probability that each candidate in our set is the target number. Because the probability of each candidate is the same (`1 / n`), this reduces to `H = -n * ((1/n) * log2(1/n))`.

We can then calculate the **information gain** of each guess, by calculating the entropy of the space *before* the guess was made and subtracting this from the entropy *after* the guess.

Finally, we can figure out what `n` (i.e. the number of candidates) *would* be if we had made the optimal guess at each decision point. So we can figure out what the entropy would be and what the optimal information gain. Therefore we can figure out what our information gain is relative to the optimal information gain. Here I've called this `infoGain.norm` as in "normalised to optimal", which is probably not the best name.

We can see that many guesses have an optimality of close to 1 (representing optimal guesses). The majority of guesses have an optimality between 0 and 1. The remaining guesses, with optimality of > 1, are "lucky guesses" (e.g. guessing "90" on attempt #1, when the target is 95, even though this would not be a good strategy in the long run).

```{r}

guesses %>%
  ggplot(aes(x = infoGain.norm)) + 
  geom_density(color=color1, size=1) +
  geom_density(data = guesses.binary,
    color=color.binary, size=1) +
  geom_density(data = guesses.random,
    color=color.random, size=1) +
  ylim(0,2)+
  theme_minimal() +
  labs(
    title="How much information did guesses gain compared to binary search?",
    x = "Information gain / IG of binary search",
    y = "Density"
  )

```

Here they are binned.

I think the NAs are interesting/worth exploring more. They seem to be caused by cases where there is only one number available to guess. The way I've calculated infoGain, there is really no "information" gained in this situation. If only one number is left, you know that's it. Maybe we can redefine things if we want to include these cases.

```{r}


guesses %>%
  mutate(ig.norm.bin = cut(infoGain.norm, breaks = c(-0.01, 0.5, 0.95, 1.05, 10),
         labels = c("Poor (<0.5)", "Fair (0.5-0.95)", "Optimal (0.95-1.05)", "Lucky (1.05+)"))
         ) %>%
  group_by(ig.norm.bin) %>%
  summarise(n=n()) %>%
  ggplot(aes(x = ig.norm.bin, y = n, fill = ig.norm.bin)) + 
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(
    title="Guesses by optimality, binned.",
    x = "Optimality",
    y = "No. guesses",
    fill = "Optimality"
  )

```


# 2. How do player strategies change over time?

## Does player performance improve?

```{r}

games %>%
  ggplot(aes(game_index, numGuesses)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  geom_hline(yintercept = guesses.random %>%
                          group_by(gameId) %>% 
                          summarise(n = n()) %>% 
                          pull(n) %>% mean(),
             color = color.random
  )+
  geom_hline(yintercept = guesses.binary %>%
                          group_by(gameId) %>% 
                          summarise(n = n()) %>% 
                          pull(n) %>% mean(),
             color = color.binary
  )+
  theme_minimal() + 
  labs(
    title="Number of guesses for n-th game of user",
    x = "No. previous games played by user",
    y = "Number of guesses")

  

```

Shows the mean number of guesses needed to complete each game against the "game index", or the number of games the user had played up until this point.

There does seem to be a downward trend but it's not as dramatic as I was expecting. Maybe this is because people who play for a long period of time are not very good (e.g. children, definition of madness etc), or because no_guesses is too weak of a measure (it can still be tough to get the number quickly even if you are using a good strategy). People who play for a long time are overall performing pretty well because they are under the average number of guesses, which is probably about as good as they can get because what number they get is dependent on random chance. Can we quantify an average number of guesses if you were always using optimal strategy for comparison?

```{r}

games %>%
  ggplot(aes(game_index, propOptimal)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  geom_hline(yintercept = guesses.random %>%
                          pull(isOptimal) %>% mean(),
             color = color.random
  )+
  geom_hline(yintercept = guesses.binary %>%
                          pull(isOptimal) %>% mean(),
             color = color.binary
  )+
  theme_minimal() + 
  ylim(0,1) +
  labs(
    title="Proportion of optimal guesses for n-th game of user",
    x = "No. previous games played by user",
    y = "Proportion of optimal guesses"
  )
  

```
Likewise, the proportion of optimal guesses markedly increases with the total number of games.

**SOMETHING WRONG W INFOGAIN HERE**
```{r}

games %>%
  ggplot(aes(game_index, infoGain.mean)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  geom_hline(yintercept = guesses.random %>%
                          pull(infoGain) %>% mean(),
             color = color.random
  )+
  geom_hline(yintercept = guesses.binary %>%
                          pull(infoGain) %>% mean(),
             color = color.binary
  )+
  theme_minimal() + 
  labs(
    title="Mean information gain per guess for n-th game of user",
    x = "No. previous games played by user",
    y = "Mean information gain"
  )
  

```
The same cannot be said for the mean information gain, however. How come?
<!-- I think somethings wrong with the information gain measure -->

Finally, let's look at whether players get faster over the course of their career?

```{r}
games %>%
  ggplot(aes(game_index, duration/numGuesses)) +
  stat_summary_bin(fun.data="mean_cl_boot",
                   geom="pointrange",
                   binwidth = 5,
                   color=color2) +
  geom_smooth(method = 'lm') + 
  theme_minimal() + 
  labs(
    title="Duration per guess of n-th game of user",
    x = "No. previous games played by user",
    y = "Duration per guess in seconds"
  )
```



```{r}

m1 <- lm(numGuesses ~ game_index, data=games)
summary(m1)

```

A linear model shows a strong relationship between numGuesses and game_index (you expect to make one fewer guess about every 300 games).

```{r}

model.game_index.guesses.null <- lmer(numGuesses ~ (1 + game_index | user), data=games)

summary(model.game_index.guesses.null)

model.game_index.guesses <- lmer(numGuesses ~ game_index + (1 + game_index | user), data=games)

summary(model.game_index.guesses)

anova(model.game_index.guesses, model.game_index.guesses.null)

```

A linear mixed effects model (which takes into account the variance between participants), shows a stronger relationship (it predicts you will make one fewer guess every 150 games). A likelihood ratio test against a null model shows the effect to be significant.

# 3. What other strategies do players use and how can we characterize them?

Potential Strategies can include picking numbers that are even or odd and rounded by 10's and 5's. 

(is there a function that we can easily select out even or odds? or do we just write a function of like N/2 and if its true, filter it? not really sure...)

This graph shows that mean guesses tend to congregate around 5's and 10's.

move mean guess graph (distributions of 5's and 10s) here perhaps?

```{r}

users %>%
  mutate(no_games_bin = cut(
        no_games,
        breaks = c(0, 1, 5, 200),
        labels = c("1", "2-4", "5+"),
        )
  ) %>%
  ggplot(aes(x=round(guesses.mean), fill=no_games_bin)) +
  geom_bar() +
  theme_minimal() +
  scale_fill_brewer(palette="Paired") +
  labs(
    title="Distribution of mean guesses per game across users",
    x = "Mean guesses per game",
    y = "Number of users",
    fill = "Games played"
  )

```


