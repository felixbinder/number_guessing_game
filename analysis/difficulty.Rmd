---
title: "R Notebook"
output:
  html_document:
    highlight: kate
    theme: flatly
    toc: yes
    toc_float: yes
    df_print: paged
---

# Summary

**TL;DR**: People are worse at finding the midpoint between two numbers when i) those numbers are large, ii) those numbers are far apart, iii) they have to "borrow" during substitution, iv) they have to "carry" during addition; v) the difference between the numbers is not divisible by 20.

## Assumptions

- People try to use binary search (find the midpoint) to play the number guessing game.
- People find the midpoint, `M` between a lower bound `L` and upper bound `U` using three arithmetic operations: subtraction (`I = U-L`); division (`J = I / 2`); and addition (`M = L + J`).
- People start by assuming their lower bound is `0` and their upper bound is `100`. When they receive feedback on a guess, they update their bounds "lazily" (i.e. if they guess `20` and Alexa says *higher*, they update their lower bound to `20`, even though they have enough information to update it to `21`).
- People can do single digit arithmetic in their head without breaking the problem down into steps. They do more complex problems by breaking them down into simpler problems.
- When a problem is harder to do, people perform worse on it (either because they make a mistake, or because they don't allocate the necessary cognitive resources to solving the problem).

## Hypotheses

We identified three categories of sources of processing difficulty, and hypothesize that each contributes to processing difficulty.

1. Problem size (larger numbers are harder to process).
2. Carrying & Borrowing (carrying and borrowing numbers creates difficulty via working memory load)
3. No. Single Digit operations (more operations increase difficulty)

## Method

We constructed measures of processing difficulty for each of the arithmetical steps assumed to comprise the binary search algorithm. For each guess which was made by a user, we estimated the processing difficulty on each of these measures based on the upper and lower bounds available to the user when the guess was made. We measured how successful the Expected Information Gain (EIG) of the user's guess relative to the EIG of the optimal guess. We call this measure Relative Expected Information Gain (REIG). We constructed linear mixed effects models to predict REIG using our measures of arithmetical difficulty.

## Results

We found support for hypotheses 1 & 2, but limited support for hypothesis 3. Problem size (as estimated by the size of the lower bound, and the interval between the lower and upper bounds) both caused a decrease in REIG. Interval size was the strongest predictor we tested.

Carrying during addition or borrowing during subtraction both caused decreases in REIG. In contrast, carrying during division (due to the dividend being odd in the tens- or ones-place) did not cause a decrease in EIG, and instead correlated with a small increase.

Finally, most of our measures which indicated additional operations would need to be done produced effects showing an *increase* in REIG. Only `J` (half of the interval between `U` and `L`) being divisible by 10 produced a strong positive effect on REIG.

## Discussion

- Strong effect of problem size
  - Both interval size and lower bound have an effect
  - Well supported in math cog lit
  
- Strong support for WM constraints
  - addition borrowing & subtraction carrying
  - Support in lit?
  
- No effect for odd division
  - Are we indexing this correctly?
  - Maybe people do division differently (especially division by 2, long division method is pro bably not efficient).

- Effect for J mod 10
  - means I is divisible by 20
  - Makes everything very easy e.g. (20, 40) or (40,60)
  - Any good analysis of why this is so?
  
- No effect of other op measures
  - Arguably surprising
  - Can anyone see any issues with the way this has been operationalised/tested
  - Maybe number of ops is not a big factor of what makes math hard/scary
  - People just as happy to do 47 - 13 as 47 - 3

## Questions

- Random slopes(?)
  - I tried including them and the model ran for 45 mins. I will run a full slopes model overnight. Hopefully its results aren't super different...
- attemptNum
  - attemptNum still has a **very** strong effect, even controlling for everything else. Can we explain why this is? Is it relevant to our hypotheses?
- Renaming REIG
  - 'Relative' does not seem appropriate here.
- condense measures?
  - 1 predictor for no. carries/borrows, 1 for no. ops

## ToDo

- Plots


# Effect of difficulty on REIG

We can characterize the optimal search strategy as an algorithm which attempts to find *M*, the mid-point between the upper bound (*U*) and the lower bound (*L*). This can be broken down into three arithmetical operations:

1. *I = U - L*
2. *J = I / 2*
3. *M = L + I*

where *I* is the interval between *U* and *L*, and *J* is half of this interval.

We can then characterize the difficulty of the operation in terms of the difficulty of applying each arithemetical operation at each step. For instance, where *L* is divisible by 10, the user does not have to make a subtraction in the *ones*-place, so we might expect step (1) to be simpler.


## Proposed measures

1. Subtraction operation

    A. As the size of *I* increases, processing cost increases (Ashcraft & Battaglia, 1978)
    
    B. If `L %% 10 == 0`, processing cost will **decrease** because no subtraction in the *ones*-place is necessary (`x - 0 = x`).
    
    C. If `L < 10`, processing cost will **decrease** because no subtraction in the *tens*-place is necessary (`x - 0 = x`)

    D. If `(U %% 10) < (L %% 10)`, processing cost will **increase** because a 1 will have to be "borrowed" from the *tens*-place.
    
    E. If `(U %/% 10) == (L %/% 10)` processing cost will **decrease** because only a simple subtraction is necessary in the tens place (`x - x = 0`). [Note: `%/%` is division without remainder (so basically if they're the same in the tens-place].
    
2. Division operation

    A. If *I* is odd, processing cost **increases**, as odd numbers are harder to divide by two.
    
    B. If the *tens*-place digit of *I* is odd, processing cost **increases**
    
    C. As *I* increases, processing difficulty **increases** (problem size, same as 1.A)
    
3. Addition operation

    A. As *lowerBound* increases, processing difficulty **increases** (problem size, related to 1.A but negatively correlated).
    
    B. As *J* (interval / 2) increases, processing difficulty **increases** (problem size, perfectly correlated with *I* so not separately included).
    
    C. If `((J %% 10) + (L %% 10)) >= 10` difficulty **increases**, as 1 must be carried.
    
    D. If `J < 10` difficulty **decreases** as no addition need be made in the *tens*-place.
    
    E. If `(J %% 10) == 0`, difficulty **decreases** as no addition must be made in the *ones*-place.
  
    F. If `L < 10` difficulty **decreases** as no addition need be made in the *tens*-place (same as 1B).
    
    E. If `(L %% 10) == 0`, difficulty **decreases** as no addition must be made in the *ones*-place (same as 1C).
    
## Setup
```{r}

# Load libraries
suppressMessages(library(tidyverse))
library(tidyverse)
library(RColorBrewer)
library(knitr)
library(lmerTest)

# Load data from data processing 
guesses <- read.csv('../data/guesses_processed.csv')
games <- read.csv('../data/games_processed.csv')

# Create color palette
paired <- brewer.pal(6, "Paired")
color1 <- paired[1]
color2 <- paired[2]
color3 <- paired[3]

color.binary = "green"
color.random = "red"
```

## Extract measures

We extract the measures described above.

```{r}

guesses <- guesses %>%
  mutate(
    REIG.inv = 1 - EIG.relative,
  
    # Subtraction measures
    interval = upperBound - lowerBound,
    lb.mod10.0 = lowerBound %% 10 == 0,
    lb.lt.10 = lowerBound < 10,
    sub.borrow = (upperBound %% 10) < (lowerBound %% 10),
    sub.tens.eq = (upperBound %/% 10) == (lowerBound %/% 10),
  
    # Division measures
    interval.odd = (interval %% 2 == 1),
    interval.tens.odd = ((interval %/% 10) %% 2) == 1,
  
    # Addition measures
    J = interval / 2,
    add.carry = ((J %% 10) + (lowerBound %% 10)) >= 10,
    j.lt.10 = J < 10,
    j.mod10.0 = (J %% 10) == 0
  )

```

We extract the same measures under the assumption that users are doing 'lazy updating' of boundaries.

```{r}
# Same calculations with adjusted bounds
guesses.adj <- guesses %>%
  mutate(
    
    # Adjusted lower bound for lazy updating (assuming represents initial lb as 0)
    lowerBound.adj = lowerBound - 1,
    
    # Adjusted upper bound for lazy updating
    upperBound.adj = case_when(
      upperBound == 100 ~ 100,  # No adjustment when U = 100
      TRUE ~ upperBound + 1  # If U has moved, increment by 1 to simulate lazy updating
    ),

    # Subtraction measures
    interval = upperBound.adj - lowerBound.adj,
    lb.mod10.0 = lowerBound.adj %% 10 == 0,
    lb.lt.10 = lowerBound.adj < 10,
    sub.borrow = (upperBound.adj %% 10) < (lowerBound.adj %% 10),
    sub.tens.eq = (upperBound.adj %/% 10) == (lowerBound.adj %/% 10),
  
    # Division measures
    interval.odd = (interval %% 2 == 1),
    interval.tens.odd = ((interval %/% 10) %% 2) == 1,
  
    # Addition measures
    J = interval / 2,
    add.carry = ((J %% 10) + (lowerBound.adj %% 10)) >= 10,
    j.lt.10 = J < 10,
    j.mod10.0 = (J %% 10) == 0
  )

```

## Linear Model 

We make a kitchen sink model with all of our measures.

```{r}

# Full model
m.all = lmer(EIG.relative ~ interval + lowerBound + lb.mod10.0 + lb.lt.10 + sub.borrow +
                        sub.tens.eq + interval.odd + interval.tens.odd + 
                        add.carry + j.lt.10 + j.mod10.0 + (1 | user),
             data=guesses, REML = F)

summary(m.all)

```

We run a second model under the assumptions that users are doing lazy bound updating

```{r}

# Adjusted model

# Full model
m.adj = lmer(EIG.relative ~ interval + lowerBound.adj + lb.mod10.0 + lb.lt.10 +
                            sub.borrow + sub.tens.eq + interval.odd + interval.tens.odd + 
                            add.carry + j.lt.10 + j.mod10.0 + (1 | user),
             data=guesses.adj, REML = F)

summary(m.adj)

```

[I tried running a model with random slopes for ~45 mins and no result. I will leave it going overnight].

```{r}

# m.adj.slopes = lmer(EIG.relative ~ interval + lowerBound + lb.mod10.0 + lb.lt.10 +
#                             sub.borrow + sub.tens.eq + interval.odd + interval.tens.odd +
#                             add.carry + j.lt.10 + j.mod10.0 +
#                (1 + interval + lowerBound + lb.mod10.0 + lb.lt.10 +
#                 sub.borrow + sub.tens.eq + interval.odd + interval.tens.odd +
#                 add.carry + j.lt.10 + j.mod10.0 | user),
#              data=guesses.adj, REML = F)

```

We compare the AIC of the adjusted and non-adjusted models. The adjusted model has a lower AIC so we decide to continue using the adjusted analysis (which is better motivated anyway).

```{r}

AIC(m.all)
AIC(m.adj)

```


# Visualisations

## Interval size

REIG tends to lower as interval size increases. With a few expected exceptions at 25, 50, 100. In general this fits with the hypothesis that it's harder to do the optimal calculation for high intervals.

Below we see the effect of interval on REIG.

```{r}

guesses.adj %>%
  ggplot(aes(interval, EIG.relative)) + 
  stat_summary(fun = "mean", geom = "point") + 
  geom_smooth(method = "lm", formula = "y ~ x", se = F) + 
  theme_minimal() + 
  labs(
    y = "REIG",
    x = "Interval"
  )

```


## Lower bound

```{r}

guesses.adj %>%
  ggplot(aes(lowerBound.adj, EIG.relative)) + 
  stat_summary(fun = "mean", geom = "point") + 
  geom_smooth(method = "lm", formula = "y ~ x", se = F) + 
  theme_minimal() + 
  labs(
    y = "REIG",
    x = "Lower Bound"
  )

```

## Borrowing

```{r}

guesses.adj %>%
  mutate(sub.borrow = ifelse(sub.borrow, "Borrowing", "No Borrowing")) %>% 
  ggplot(aes(sub.borrow, EIG.relative, fill = sub.borrow)) + 
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar", width = 0.2, alpha = 0.5) + 
  theme_minimal() + 
  scale_fill_brewer(palette = "Paired") + 
  theme(
    legend.position = "none"
  ) +
  labs(
    y = "REIG",
    x = "Borrowing required for substitution"
  )

```

## Carrying

```{r}

guesses.adj %>%
  mutate(add.carry = ifelse(add.carry, "Carrying", "No Carrying")) %>% 
  ggplot(aes(add.carry, EIG.relative, fill = add.carry)) + 
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar", width = 0.2, alpha = 0.5) + 
  theme_minimal() + 
  scale_fill_brewer(palette = "Paired") + 
  theme(
    legend.position = "none"
  ) +
  labs(
    y = "REIG",
    x = "Carrying required for addition"
  )

```

# Effect of attemptNum

We augment the adjusted model above with `attemptNumReversed` as a predictor and find that it has a very strong predictive effect, even controlling for the other predictors.

[I'm not quite sure how to interpret this as I assumed a lot of its effect would be explained by interval.]

```{r}

# Add reversed index
guesses.adj <- merge(
  guesses.adj, games %>% select(gameId, numGuesses), by="gameId"
  ) %>%
  mutate(
    attemptNumReversed = numGuesses - (attemptNum - 1),
  )

# Full model
m.adj.an = lmer(EIG.relative ~ interval + lowerBound + lb.mod10.0 + lb.lt.10 +
                            sub.borrow + sub.tens.eq + interval.odd + interval.tens.odd + 
                            add.carry + j.lt.10 + j.mod10.0  + attemptNumReversed + (1 | user),
             data=guesses.adj, REML = F)

summary(m.adj.an)

anova(m.adj.an, test="LRT")

```

# Heatmap analysis

In a heatmap of no. guesses by bounds, we can see more guesses are made from bounds divisible by 10 (adjusted by 1 as discussed above).

```{r}

guesses.bounds <- guesses.adj %>%
  group_by(lowerBound.adj, upperBound.adj) %>%
  summarise(n = n(),
            EIG.relative = mean(EIG.relative),
            .groups="drop")


guesses.bounds %>%
  ggplot(aes(x = lowerBound.adj, y = upperBound.adj, fill = log10(n))) +
  geom_raster() + 
  theme_minimal() + 
  labs(
    title = "Frequency of intervals",
    fill='Frequency (log)'
  )+
  xlab("Lower bound")+
  ylab("Upper bound")

```

My main takeaway from this is that poor guesses are concentrated in this 'inner triangle' where the interval is relatively high. This could be partly because high intervals are harder and partly because getting yourself into a situation where (1,98) are your bounds requires being a non-rational user

```{r}

guesses.bounds %>%
  ggplot(aes(x = lowerBound.adj, y = upperBound.adj, fill = EIG.relative)) +
  geom_raster() + 
  theme_minimal() + 
  scale_fill_distiller(palette="RdYlGn", direction = 1) +
  labs(
    title = "rEIG per interval",
    fill='rEIG'
  )+
  xlab("Lower bound")+
  ylab("Upper bound")
```

Alternate view with lower bound and interval.

```{r}


guesses.adj %>%
  group_by(lowerBound.adj, interval) %>%
  summarise(n = n(),
            EIG.relative = mean(EIG.relative),
            .groups="drop") %>%
  ggplot(aes(x = lowerBound.adj, y = interval, fill = EIG.relative)) +
  geom_raster() + 
  theme_minimal() + 
  scale_fill_distiller(palette="RdYlGn", direction = 1) +
  labs(
    title = "rEIG per lower bound and interval size",
    fill='rEIG'
  )+
  xlab("Lower bound")+
  ylab("Interval size")


```


# Appendix

[I've added the guess index stuff to an appendix because I don't think it's as theoretically motivated as our other measures.]

## REIG by Guess Index

Relative EIG starts relatively high, seems to rise until ~ guess 5, then drops down at higher guess counts.

I think the drop toward the end could be due to "good" guessers finishing early and not reaching that stage of their game.

```{r}

guesses %>%
  ggplot(aes(attemptNum, EIG.relative)) + 
  stat_summary(fun = "mean", geom = "point")

```

```{r}

# Add reversed index
guesses <- merge(
  guesses, games %>% select(gameId, numGuesses), by="gameId"
) %>%
  mutate(
    attemptNumReversed = numGuesses - (attemptNum - 1),
  )

```

We can test this in a way by introducing a "reversed" index. Here we can see that "late" guesses (closer to final guess) are generally better than "early" guesses. This implies both that a) late guesses are better for everyone and b) very bad players (who take up to 12 guesses) unsurprisingly have a lower REIG on their early guesses.

The late guess effect could be due to the index being smaller, which we test below.

```{r}
guesses %>%
  ggplot(aes(attemptNumReversed, EIG.relative)) + 
  stat_summary(fun = "mean", geom = "point")

```

## Interval size

A likelihood ratio test comparing linear mixed effects models with random intercepts by user showed a strong negative effect of interval size (p < 0.001) -- good news!

[Note1: models with random *slopes* for numAvailable failed to converge. If we think random slopes are theoretically motivated we can try to get them to converge with another model solving package.

Note2: I haven't used the lmerTest package Ed recommended in lecture because I'm in a rush, but I will go back and do that later and it shouldn't alter our results.]

```{r}

m.null = lmer(EIG.relative ~ (1 | user), data=guesses, REML = F)

m.1A = lmer(EIG.relative ~ numAvailable + (1 | user), data=guesses, REML = F)

summary(m.1A)

anova(m.1A, m.null)

```

## Effect of "difficult" intervals

Here are the "easiest" intervals (highest REIG). Although maybe we should be using proportion of optimal guesses here? It depends what we're trying to say about what successfully deploying the strategy looks like.

```{r}

# Example guesses

# guesses %>%
  # filter(guess == 50)
  # filter(gameId == "0005a359-0253-4b71-a7b4-3ab47f2ef54d")

guesses %>%
  group_by(numAvailable) %>%
  summarise(EIG.relative = mean(EIG.relative)) %>%
  arrange(desc(EIG.relative)) %>%
  filter(dense_rank(EIG.relative) > 89) %>%
  ggplot(aes(reorder(factor(numAvailable), -EIG.relative), EIG.relative)) + 
    geom_bar(stat="identity", fill=color1) + 
  theme_minimal()


```

Because of the way the splits work, it looks like the easiest numbers are either *on*
a 5 or a 10 (e.g. bounds like [1,100] or [5,10]), or 1 below a 5 or a 10 (e.g. [1,10], [6,10]).

We can create a variable that tracks this.

```{r}

guesses <- guesses %>%
  mutate(
    intervalType = case_when(
      numAvailable %% 10 == 0 ~ "x0",
      numAvailable %% 5 == 0 ~ "x5",
      (numAvailable + 1) %% 10 == 0 ~ "x9",
      (numAvailable + 1) %% 5 == 0 ~ "x4",
      numAvailable == 2 ~ "2",
      TRUE ~ "Other"
    )
  )


guesses %>%
  ggplot(aes(intervalType, EIG.relative, fill=intervalType)) + 
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar") + 
  theme_minimal() + 
  scale_fill_brewer(palette="Set2")
```

I'm surprised that Other is not lower here. Let's have a look at them against the interval size.

```{r}

guesses %>%
  ggplot(aes(numAvailable, EIG.relative, color=intervalType)) + 
  stat_summary(fun = "mean", geom = "point", mapping=aes(color=intervalType)) + 
  # geom_smooth(method = "lm", formula = "y ~ x", se = F) + 
  scale_color_brewer(palette="Set2") +
  theme_minimal()

```

With best fit lines.

All have a downward slope other than x0 intervals which are unaffected (probably due to the 100 interval on the right being so optimal)

```{r}

guesses %>%
  ggplot(aes(numAvailable, EIG.relative, color=intervalType)) + 
  stat_summary(fun = "mean", geom = "point", mapping=aes(color=intervalType)) + 
  geom_smooth(method = "lm", formula = "y ~ x", se = F) +
  scale_color_brewer(palette="Set2") +
  theme_minimal()

```


## 1B: Lower bound mod 10 == 0

> Might be worth to also run the divisibility analysis not only on the size of the interval, but also on the start (or end) of the interval. Like, the midpoint of [20,28] is obvious, the midpoint of [23,31] is not (to me at least). I’d expect that intervals of any size where the lower bound is divisible by 10 lead to higher rEIG.

Let's look at how mod10 of the start interval affect rEIG on the subsequent guess. 0 here means either 10, 20,..., 7 means 17,47,...

```{r}
guesses %>% mutate(mod10 = lowerBound %% 10) %>% ggplot(aes(mod10, EIG.relative)) +
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar") +
  stat_summary(aes(label=round(..x..,2)), fun.y=mean, geom="text", size=6,
             vjust = -0.5)+
  theme_minimal()

```

Doesn't look like much of an effect. Interestingly, _0 is lower than others, even though calculating the midpoint of the interval shold be easiest here. Perhaps this is because we only land on _0 if the prior guess was really good already? Most of these might are likely going to be 50.

Let's try filtering out the first two guesses to see if guessing 50 on first guess was driving the poor showing of _0. 

```{r}
guesses %>% filter(attemptNum > 2) %>% mutate(mod10 = lowerBound %% 10) %>% ggplot(aes(mod10, EIG.relative)) +
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar") +
  stat_summary(aes(label=round(..x..,2)), fun.y=mean, geom="text", size=6,
             vjust = -0.5)+
  theme_minimal()

```
Doesn't look like the poor showing for _0 is because of 50 as first guess. 

Let's try to aggregate like above, only for interval start instead of size. Note that eg. 10 only counts as x10, not as even! 

```{r}

guesses %>%
  mutate(
    lowerBoundType = case_when(
      lowerBound %% 10 == 0 ~ "x0",
      lowerBound %% 5 == 0 ~ "x5",
      (lowerBound + 1) %% 10 == 0 ~ "x9",
      (lowerBound + 1) %% 5 == 0 ~ "x4",
      lowerBound == 2 ~ "2",
      lowerBound %% 2 == 0 ~ "even",
      TRUE ~ "Other"
    )
  ) %>% 
  ggplot(aes(lowerBoundType, EIG.relative, fill=lowerBoundType)) + 
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar") + 
  theme_minimal() + 
  scale_fill_brewer(palette="Set2")
```
Again, we see that people do poorly on the _0s.


This shows us how much information we get relative to using the optimal strategy. Let's look at the percentage of guesses **exactly following the optimal strategy**.

```{r}
guesses %>% mutate(mod10 = lowerBound %% 10) %>% ggplot(aes(mod10, prob.optimal)) +
  stat_summary(fun = "mean", geom = "bar") + 
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar") + 
  stat_summary(aes(label=round(..x..,2)), fun.y=mean, geom="text", size=6,
             vjust = -0.5)+
  theme_minimal()

```
Interesting, wonder why _1 does so poorly.

### Linear model

The linear model shows an effect in the opposite direction to that we would expect (i.e. a lower bound of 0 **decreases** optimality and hence **increases** processing cost).

```{r}

guesses <- guesses %>%
  mutate(l.mod10 = factor(ifelse(lowerBound %% 10 == 0, 1, 0)),
         l.mod10.adj = factor(ifelse((lowerBound + 1) %% 10 == 0, 1, 0)))

m.1B = lmer(EIG.relative ~ l.mod10 + (1 | user), data=guesses, REML = F)

summary(m.1B)

anova(m.1B, m.null)

```

Let's run the adjusted version as well, where we assume users don't update their bound correctly, so they think the lower bound is 'x0' when it's actually 'x1'.

The model also shows a decrease in optimality for the adjusted bound.

```{r}

m.1B.adj = lmer(EIG.relative ~ l.mod10.adj + (1 | user), data=guesses, REML = F)

summary(m.1B.adj)

anova(m.1B.adj, m.null)

```


## 1C: Lower Bound < 10

Contrary to our hypotheses above, a lower bound of less than 10 leads to sub-optimal performance.

It's worth thinking about how this will covary with interval size and whether some of these measures would perform better in a combined model.

```{r}

guesses <- guesses %>%
  mutate(
    l.lt10 = factor(ifelse(lowerBound < 10, 1, 0))
  )

guesses %>%
  ggplot(aes(x = l.lt10, y = EIG.relative)) + 
  stat_summary(fun="mean", geom = "bar") + 
  stat_summary(fun.data="mean_cl_boot", geom = "errorbar") + 
  theme_minimal()


```

```{r}

m.1C = lmer(EIG.relative ~ l.lt10 + (1 | user), data=guesses, REML = F)

summary(m.1C)

anova(m.1C, m.null)

```
