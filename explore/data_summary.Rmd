---
title: "Preliminary Data Summary | Number Guessing Game"
author: "Cameron Jones, Felix Binder, Crystal Poole, Naomi Lin, & Robert Kaufman"
date: "15/11/2020"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: kate
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# --- Setup --- #

# Load libraries
library(tidyverse)
library(RColorBrewer)
library(knitr)

# Load data
games <- read.csv("../data/games.csv")

# Cast startTime and endTime to POSIX
games <- games %>%
  mutate(
    startTime = as.POSIXct(startTime),
    endTime = as.POSIXct(startTime)
  )

# Create color palette
paired <- brewer.pal(6, "Paired")
color1 <- paired[1]
color2 <- paired[2]
color3 <- paired[3]

```

# 1. Introduction

## 1.1. The dataset

The dataset comes from a number guessing game running on Amazon Alexa. Users play a game with the voice assistant where the game randomly chooses a number between 1 to 100 and the user tries to guess the number. After a guess, the user is told whether their guess was too low or too high. The dataset includes 380,000 guesses across 50,000 games and 14,000 players. The dataset was sourced from [kaggle](https://www.kaggle.com/sdobson46/higher-or-lower-game). The dataset includes unique player IDs for all games as well as the date of the game and the outcome of the game. The dataset also allows for analysis of a player’s first game and information about whether the player returned to play more games. This dataset only includes completed games with no more than 15 guesses. The dataset excludes all guesses that Alexa did not understand or were invalid because they were not in the 1 to 100 number range. The dataset includes repeated guesses and guesses that conflict with previous hints provided by Alexa.

### Codebook

Dataframe `games` from `games.csv` file on kaggle.

* 1 row per game played
* 50881 rows total
* 22 columns:
  * `gameId` (factor w/ 50881 levels): Unique identifier for each game
  * `user` (int): Unique identifier for each player
  * `startTime` and `finishTime` (POSIX datetime): start and finish times of games.
  * `duration` (int): Duration of game in seconds.
  * `targetNum` (int): The randomly selected number between 1-100 that the player needs to guess
  * `numGuesses` (int): Number of guesses the player needed to guess the target number
  * `guess1` - `guess15` (int): The numbers which the user guessed.
    

## 1.2 Research questions

We briefly review our research questions to make clear the relevance of specific variables and transformations.

* How well do players approximate the optimal strategy (binary search)?
  * This simple game has an optimal solution strategy, which is binary search. What proportion of users adopt this strategy? How does participant performance relate to this optimal benchmark?

* What other strategies do players use and how can we characterize them?
  * Are there other patterns in players' guesses that imply other strategies?
  * How can we identify and categorize these strategies?
  * Are strategies reliable and rational? Or do they expose irrational biases in players' reasoning?

* How do players’ strategies change and/or improve over time?
  * Do players' responses improve over time (as measured by the information gain of each guess)?
  * Do players adopt or better approximate binary search over time or do they improve by adopting other strategies?
  
* Does players’ success affect their likelihood to continue playing?
  * Do players who do very well continue to play for longer? Or do they stop quickly after improving their strategy or adopting binary search?
  
  
# 2. Data transformations

In order to make inferences about some of our research questions, we transform the data in several ways.

* Grouping data by different variables in order to make comparisons across aggregates.
  * Grouping games by user so we can look at a players' mean performance, the total number of games they played, and the time between their games.
  
* Pivoting data so that each guess is on a separate row of the dataframe.
  * This allows us to enrich data about each guess by calculating e.g. the information gain of each guess
  
* Creating new variables
  * Extracting new attributes from existing variables or relationships between them (e.g. the order in which several games were played by one participant).
  
## 2.1. Pivot to guesses

We pivot the data longer so that each guess is on a new row. We remove any rows which contain unused guesses (i.e. NA).

We then have a second dataframe `guesses`, with:
  
* One row per guess made.
* 384053 rows total

```{r}

guesses <- games %>%
  # Pivot longer on guess columns
  pivot_longer(cols = starts_with("guess"), names_to = "attemptNum", values_to = "guess",
               names_pattern = "guess([[:digit:]]+)") %>%
  
  # Filter NAs
  filter(
    !is.na(guess)
  ) %>%
  # Remove irrelevant vars
  select(gameId, user, targetNum, attemptNum, guess) %>%
  
  # Convert attemptNum to numeric
  mutate(
    attemptNum = as.numeric(attemptNum),
    guess = as.numeric(guess)
  )

```

Next we create a variety of variables to measure the *effectiveness* of each guess in different ways (i.e. how much does the guess reduce uncertainty about the target number)?

We create the following variables:

* `response` (factor w/ 3 levels): Whether targetNum is "lower" or "higher" than the guess, or "correct".
* `lowerBound` and `upperBound` (int): Minimum and maximum possible values for `targetNum` given previous guesses.
* `numAvailable` (int): The number of possible options within bounds.
* `prob` (num): The probability of randomly choosing `targetNum` from guesses within bounds [0,1].
* `entropy` (num): The level of uncertainty of `targetNum` given bounds (`-1 * numAvailable * prob * log(prob)`).
* `infoGain` (num): The reduction in `entropy` from `guess`.
* `optimalGuess` (int): The optimal guess based on binary search.
* `outOfBounds` (int): Whether the guess is within (0) or outwith (1) bounds.
* `numExcluded` (int): The number of potential choices ruled out by the `guess`.
* `propExcluded` (num): The proportion of potential choices ruled out by `guess`.

```{r}

guesses <- guesses %>%
  group_by(gameId) %>%
  mutate(guessId = paste(gameId, attemptNum, sep = "_")) %>%
  arrange(gameId, attemptNum) %>%
  mutate(
    
    # Check if guess is too high, low, or correct
    response = case_when(
      guess > targetNum ~ "lower",
      guess < targetNum ~ "higher",
      guess == targetNum ~ "correct",
    ),
    
    # Initialise lowerBound
    lowerBound = 1,
    
    # add lower and upper bound info
    lowerBound = case_when(
      row_number() == 1 ~ 1,
      lag(response) == "lower" ~ lag(lowerBound, order_by=attemptNum),
      lag(response) == "higher" ~ lag(guess, order_by=attemptNum) + 1
    ),
    
    lowerBound = cummax(lowerBound),
    
    # Initialise upperBound
    upperBound = 100,
    
    # add lower and upper bound info
    upperBound = cummin(case_when(
      row_number() == 1 ~ 100,
      lag(response) == "higher" ~ lag(upperBound, order_by=attemptNum),
      lag(response) == "lower" ~ lag(guess, order_by=attemptNum) - 1
    )),
    
    numAvailable = (upperBound - lowerBound) + 1,
    prob = 1 / numAvailable,
    
    # Get entropy
    entropy = -1 * prob * log(prob) * numAvailable,
    
    # info gain
    infoGain = entropy - lead(entropy, default = 0),
    
    # Find optimal guess
    optimalGuess = floor((upperBound + lowerBound) / 2),
    
    # check if guess is optimal
    isOptimal = as.numeric((abs(guess - ((upperBound + lowerBound) / 2))) < 1),
    
    # check if guess is out of bounds
    outOfBounds = as.numeric(guess < lowerBound | guess > upperBound),
    
    # find no excluded
    numExcluded = numAvailable - lead(numAvailable, default = 1),
    propExcluded = numExcluded / numAvailable,
    
    # Add a thousandth so we can scale plots more readably
    mili = 1 / 1000,
  ) %>%
  ungroup()

```

`infoGain` and `propExcluded` are highly correlated, but the latter is more interpretable.

```{r}

cor(guesses$infoGain, guesses$propExcluded)


```
  
  
We then group the guess data by `gameId` once again to compute aggregate statistics across games (e.g. mean information gain), and join this data onto the original `games` dataset. Finally we group games by user and rank them based on `startTime` in order to determine the game's index - the order in which participant played games.

We add the following column to games:

* `infoGain.mean` (num): The mean information gain of guesses in the game.
* `propExcluded.mean` (num): The mean number of options excluded by guesses in the game.
* `numOptimal` (num): The number of optimal guesses in the game.
* `propOptimal` (num): The proportion of guesses that were optimal in the game.
* `game_index` (int): The rank of games in order of `startTime` by user.

```{r}

# Group guesses by gameId and calculate aggregate stats
games.info <- guesses %>%
  group_by(gameId) %>%
  summarise(
    infoGain.mean = mean(infoGain),
    propExcluded.mean = mean(propExcluded),
    numOptimal = sum(isOptimal),
    propOptimal = numOptimal / n(),
    .groups = "drop"
  )

# Merge games.info back into games
games <- merge(games, games.info) 

# Add game_index information
games <- games %>%
  group_by(user) %>%
  mutate(
    game_index = dense_rank(startTime)
  ) %>%
  ungroup()

```

  
## 2.2. Group by user

We group games and guesses by user to create anothe dataframe, `users`:

* Each row now represents one user
* 14782 rows total.

We then extract several variables.

From `games` we extract:

* `user` (int): The unique identifier of the user.
* `no_games` (int): The number of games played by the user.
* `firstGame` and `lastGame` (POSIX): The start times of the users' first and last games.
* `career_length` (num): The difference in time between `lastGame` and `firstGame` in hours.
* `duration.mean` (num): The mean duration of this users' games.
* `guesses.mean` (num): The mean number of guesses the user made in games.
* `guesses.total` (num): The total number of guesses made by the user across all games.
* `no_games_bin` (factor w/5 levels): Number of games played binned into {1, 2-4, 5-9, 10-99, 100+}.

From `guesses`, we extract:

* `propExcluded.mean` (num): The mean number of choices excluded by guesses across a user's games.
* `infoGain.mean` (num): The mean information gain of guesses across a user's games.
* `numOptimal` (num): The number of optimal guesses across a user's games.
* `propOptimal` (num): The proportion of guesses that were optimal across a user's games.


```{r}

# Create a summary df grouped by user
users <- games %>% 
  group_by(user) %>%
  summarise(
    # Number of games played
    no_games=n(),
    # Date of first game
    first_game=min(startTime),
    # Date of last game
    last_game=max(startTime),
    # Mean duration of games
    duration.mean=mean(duration),
    # Mean no_guesses
    guesses.mean=mean(numGuesses),
    guesses.total = sum(numGuesses),
    .groups = "drop"
  ) %>%
  mutate(
    # bin no_games into roughly even chunks
    no_games_bin=cut(
      no_games,
      breaks = c(0, 1, 2, 5, 10, 200),
      labels = c("1", "2-4", "5-9", "10-99", "100+"),
      ),
    # find the time between first and last games
    career_length = difftime(last_game, first_game, units="hours"),
    career_length_bin = factor(
      case_when(
        no_games < 2 ~ "Single game",
        career_length < (5 / 60) ~ "< 5 min",
        career_length < 1 ~ "5-60 min",
        career_length < 24 ~ "1-24 hr",
        career_length < 168 ~ "1-7 days",
        career_length < 720 ~ "7 - 30 days",
        career_length >= 720 ~ "30+ days"),
    levels = c("Single game", "< 5 min", "5-60 min", "1-24 hr", "1-7 days", "7 - 30 days", "30+ days")
    )
  )


users <- merge(
  users,
  guesses %>%
    group_by(user) %>%
    summarise(
      infoGain.mean = mean(infoGain),
      propExcluded.mean = mean(propExcluded),
      numOptimal = sum(isOptimal),
      propOptimal = numOptimal / n(),
      .groups = "drop"
    )
)

```

# 3. Distribution of data

### 3.1. Guesses dataset

We now visualise our variables to see how they are distributed, starting with the pivoted `guesses` data.

Firstly, we use the summary function to get an overview of how our variables are distributed.

```{r}

summary(guesses)
```

#### Distribution of response

The majority of responses to guesses are "higher" and "lower" as one would expect, with only ~13% of guesses being correct. Users appear to guess too low slightly more frequently than too high

```{r}

guesses %>%
  group_by(response) %>%
  summarise(prop = n() / nrow(guesses), .groups = "drop") %>%
  ggplot(aes(x = response, y = prop, fill = response)) + 
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_brewer(palette = "Paired") +
  labs(
    title="Distribution of guess responses",
    x = "Response",
    y = "Proportion of guesses",
    fill = "Response"
  )

```

#### Distribution of guesses

The distribution of guesses itself is very uneven, with users clearly favoring some numbers arbitrarily over others. The dashed line represents the frequency you would expect for each number by chance.

```{r}

guesses %>%
  group_by(guess) %>%
  summarise(prop = n() / nrow(guesses), .groups = "drop") %>%
  ggplot(aes(x = factor(guess), y = prop)) + 
  geom_hline(yintercept = 0.01, linetype = "dashed", alpha = 0.4, color = "black") +
  geom_bar(fill=color1, stat = "summary", fun = "sum") +
  theme_minimal() +
  labs(
    title="Distribution of guesses",
    x = "Guess",
    y = "Proportion of guesses"
  ) + 
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x = element_blank()
  )

```


Sorting numbers by number of guesses, we can see that users are favouring round numbers (multiples of 5 and 10). However, some of these numbers are also consistent with binary search (e.g. `50`, `25`).

```{r}

guesses %>%
  group_by(guess) %>%
  summarise(
    prop = n() / nrow(guesses),
    .groups = "drop"
  ) %>%
  arrange(desc(prop)) %>%
  filter(
    row_number() < 11
  ) %>%
  ggplot(aes(x = reorder(factor(guess), -prop), y = prop)) + 
  geom_bar(fill=color1, stat = "identity") +
  geom_hline(yintercept = 0.01, linetype = "dashed", alpha = 0.4, color = "black") +
  theme_minimal() +
  labs(
    title="Top 10 most guessed numbers",
    x = "Number",
    y = "Proportion of guesses"
  ) + 
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  )

```

#### Proportion of choices excluded by guess

The proportion of options excluded by each guess has two peaks: one near zero, likely indicating poor performance or out-of-bounds guesses. The other, at 0.5, might indicate use of the binary search strategy (as this is roughly the proportion we expect to reject using it).

```{r}

guesses %>%
  ggplot(aes(x = propExcluded)) + 
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of choices excluded by guess",
    x = "Proportion of choices excluded",
    y = "Density"
  )

```

#### Information gain

Information gain of responses is distributed similarly to `propExcluded` and the peaks in the distribution probably indicate similar things.

```{r}

guesses %>%
  ggplot(aes(x = infoGain)) + 
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of information gain",
    x = "Information gain",
    y = "Density"
  )

```

#### Out of bounds

Just under 10% of guesses were out of bounds, which might reflect inexperienced or young players, or difficulty in keeping track of what the current bounds are.

```{r}

guesses %>%
  mutate(outOfBounds = factor(ifelse(outOfBounds == 1, "Yes", "No"))) %>%
  group_by(outOfBounds) %>%
  summarise(prop = n() / nrow(guesses),
            .groups = "drop") %>%
  ggplot(aes(x = outOfBounds, y = prop, fill=outOfBounds)) + 
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_x_discrete(labels = c("No", "Yes")) +
  scale_fill_discrete(labels = c("No", "Yes")) +
  scale_fill_brewer(palette = "Paired") +
  labs(
    title="What proportion of guesses were out of bounds?",
    x = "Out of bounds",
    y = "Proportion of guesses",
    fill = "Out of bounds"
  )

```

#### Optimal guesses

Roughly 1/3 of guesses were optimal

```{r}

guesses %>%
  mutate(isOptimal = factor(ifelse(isOptimal == 1, "Optimal", "Sub-optimal"))) %>%
  group_by(isOptimal) %>%
  summarise(prop = n() / nrow(guesses),
            .groups = "drop") %>%
  ggplot(aes(x = isOptimal, y = prop, fill=isOptimal)) + 
  geom_bar(stat = "identity") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(
    title="What proportion of guesses were optimal?",
    x = "Optimality",
    y = "No. guesses",
    fill = "Optimality"
  )

```

### 3.2. Game data

Next we look at variables from the perspective of games.

```{r}

summary(games)

```

#### Duration

Game durations have a long upper tail: it may be worth rescaling duration logarithmically or removing outliers if we are going to do much analysis with it.

```{r}

games %>%
  ggplot(aes(x = duration)) + 
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of game durations",
    x = "Duration (s)",
    y = "Density"
  )

```

#### Number of Guesses

Guesses per game looks fairly normally distributed.

```{r}

games %>%
  ggplot(aes(x = factor(numGuesses))) + 
  geom_bar(fill=color1) +
  theme_minimal() +
  labs(
    title="Distribution of number of guesses per game",
    x = "Number of guesses",
    y = "Number of games"
  )

```

### 3.3. User

Finally, we look at how our data is distributed when it is aggregated by user.

```{r}

summary(users)

```

#### No. games per user

The majority of users played just 1-4 games, while a small number of users (~800) played >100 games. This isn't that surprising, but worth bearing in mind as any investigation of the effects of learning might be heavily skewed by this small group.

```{r}

users %>%
  ggplot(aes(x=no_games_bin)) +
  geom_bar(fill=color1) +
  theme_minimal() +
  labs(
    title="Distribution of games per player",
    x = "No. games played",
    y = "No. players"
  )

```

#### Mean guesses per user

Mean guesses per game is fairly normally distribted across users, with a mean at about 7.5 guesses.

```{r}

users %>%
  ggplot(aes(x=guesses.mean)) +
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of mean guesses per game across users",
    x = "Mean guesses per game",
    y = "Density"
  )

```


#### Mean duration

As for the by-guess aggregation, durations have a heavy upper tail.

```{r}

users %>%
  ggplot(aes(x=duration.mean)) +
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of mean game duration across users",
    x = "Mean duration in seconds per game",
    y = "Density"
  )

```

#### Career length

As with number of games per user, we have a very skewed distribution of career lengths: The majority of users played just one game, or a few games within 5 minutes of each other, yet around 800 users continued playing for over a month.

```{r}

users %>%
  ggplot(aes(x=career_length_bin)) +
  geom_bar(fill=color1) +
  theme_minimal() +
  labs(
    title="Distribution of user career lengths",
    x = "Career length",
    y = "No. players"
  )

```

#### Average proportion excluded

The majority of players fall into a neat unimodal distribution with a mean at around 0.4. However, there are several outliers averaging over 70% of choices excluded per guess.

```{r}

users %>%
  ggplot(aes(x=propExcluded.mean)) +
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of mean proportion of choices excluded per guess across users",
    x = "Mean proportion of choices excluded per guess",
    y = "Density"
  )

```

#### Information gain

As usual the information gain and `propExcluded` distributions look similar.

```{r}

users %>%
  ggplot(aes(x=infoGain.mean)) +
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Distribution of mean information gain per guess across users",
    x = "Mean information gain per guess",
    y = "Density"
  )

```

#### Proportion of optimal guesses

A unimodal distribution with a mean at around 0.25, shows most users produce an optimal guess around a quarter of the time. However, a small number of users are consistently producing optimal guesses more than 80% of the time.

```{r}

users %>%
  ggplot(aes(x=propOptimal)) +
  geom_density(color=color1, size=1) +
  theme_minimal() +
  labs(
    title="Proportion of guesses which were optimal by user",
    x = "Proportion of guesses that were optimal",
    y = "Density"
  )

```

In order to rule out users who are just getting lucky, we can filter for users who maintained > 0.8 `propOptimal` across >1 games and > 8 guesses. There are 40 such players, a small number of whom have played many games across a long period of time. Most, as in the rest of the dataset, have just played a few games over the course of minutes.

```{r}

top.users <- users %>%
  filter(propOptimal > 0.8,
         no_games > 1,
         guesses.total > 8) %>%
  arrange(desc(propOptimal))

top.users %>% head(10) %>% select(user, no_games, guesses.mean, guesses.total,
                                  career_length_bin, infoGain.mean, propExcluded.mean, 
                                  propOptimal) %>% kable()

```

```{r}

top.users %>%
  summary()

```
